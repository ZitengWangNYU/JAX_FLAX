{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flax.linen.scan module can be used to reduce the whole repeated or iterative computation graph into a single interation, therefore reduces the graph memory and compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules so we can edit the code and have it automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import timeit\n",
    "\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def measure_time(fn, params, input, num_runs=5):\n",
    "  total_time = timeit.timeit(\n",
    "    lambda: jax.tree.map(lambda x: x.block_until_ready(), fn(params, input)),\n",
    "    number=num_runs\n",
    "  )\n",
    "  avg_time = total_time / num_runs\n",
    "  if avg_time < 1e-3:\n",
    "    return f\"{avg_time * 1e6:.2f} Î¼s\"\n",
    "  elif avg_time < 1:\n",
    "    return f\"{avg_time * 1e3:.2f} ms\"\n",
    "  else:\n",
    "    return f\"{avg_time:.2f} s\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 32\n",
    "HIDDEN_DIM = 1024\n",
    "LAYERS = 42\n",
    "NUM_HEADS = 4\n",
    "DTYPE = jnp.float32\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "dummy_input = jax.random.normal(rng, (1, SEQ_LEN, HIDDEN_DIM))\n",
    "input = jax.random.normal(rng, (BATCH_SIZE, SEQ_LEN, HIDDEN_DIM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic iterative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'layer_0': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_1': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_10': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_11': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_12': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_13': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_14': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_15': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_16': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_17': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_18': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_19': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_2': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_20': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_21': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_22': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_23': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_24': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_25': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_26': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_27': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_28': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_29': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_3': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_30': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_31': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_32': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_33': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_34': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_35': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_36': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_37': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_38': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_39': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_4': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_40': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_41': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_5': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_6': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_7': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_8': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_9': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask=None):\n",
    "        out = {}\n",
    "        out[\"input_hidden_state\"] = x\n",
    "        x += nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "\t\t\t\t\t)(x, attention_mask)\n",
    "        out[\"+self_attention\"] = x\n",
    "        x += nn.Dense(features=self.hidden_dim)(x)\n",
    "        out[\"+dense\"] = x\n",
    "        return x, out\n",
    "\n",
    "class MyIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [\n",
    "            SimpleBlock(\n",
    "                name=f\"layer_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "\t\t\t\t\t\t) \n",
    "            for i in range(self.num_layers)  # Use num_layers instead of hardcoded 3\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"layer_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n",
    "model = MyIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "params = model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for regular apply: 1.07 s\n",
      "Time for first JIT call: 6.17 s\n",
      "Average time for subsequent JIT calls: 3.07 ms\n"
     ]
    }
   ],
   "source": [
    "# JIT compile the model's apply function for faster execution\n",
    "jit_apply = jax.jit(model.apply)\n",
    "\n",
    "print(f\"Average time for regular apply: {measure_time(model.apply, params, input, num_runs=5)}\")\n",
    "print(f\"Time for first JIT call: {measure_time(jit_apply, params, input, num_runs=1)}\")\n",
    "print(f\"Average time for subsequent JIT calls: {measure_time(jit_apply, params, input, num_runs=5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'blocks': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyScanModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "      self.blocks = nn.scan(\n",
    "        SimpleBlock,\n",
    "        variable_axes={},  # no scan over params\n",
    "        variable_broadcast=\"params\",  # share params\n",
    "        split_rngs={\"params\": False},  # same rng for all\n",
    "        in_axes=0,  # assume input is (layers, ...)\n",
    "        out_axes=0,\n",
    "        length=self.num_layers,\n",
    "      )(\n",
    "        hidden_dim=self.hidden_dim,\n",
    "        num_heads=self.num_heads,\n",
    "        dtype=self.dtype\n",
    "      )\n",
    "\n",
    "    def __call__(self, x):\n",
    "      x, scan_out = self.blocks(x)\n",
    "      return x, scan_out\n",
    "\n",
    "scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "scan_params = scan_model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, scan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for regular apply: 538.27 ms\n",
      "Time for first JIT call: 457.71 ms\n",
      "Average time for subsequent JIT calls: 1.43 ms\n"
     ]
    }
   ],
   "source": [
    "jit_scan_apply = jax.jit(scan_model.apply)\n",
    "\n",
    "print(f\"Average time for regular apply: {measure_time(scan_model.apply, scan_params, input, num_runs=5)}\")\n",
    "print(f\"Time for first JIT call: {measure_time(jit_scan_apply, scan_params, input, num_runs=1)}\")\n",
    "print(f\"Average time for subsequent JIT calls: {measure_time(jit_scan_apply, scan_params, input, num_runs=5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Distributed LLM inference with KV cachingm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
