{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flax.linen.scan module can be used to reduce the whole repeated or iterative computation graph into a single interation, therefore reduces the graph memory and compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-06-10 21:18:10,735:jax._src.xla_bridge:795: A Google TPU may be present on this machine, but either a TPU-enabled jaxlib or libtpu is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload modules so we can edit the code and have it automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from flax import core\n",
    "\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "SEQ_LEN = 32\n",
    "HIDDEN_DIM = 512\n",
    "LAYERS = 42\n",
    "NUM_HEADS = 4\n",
    "DTYPE = jnp.float32\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "dummy_input = jax.random.normal(rng, (1, SEQ_LEN, HIDDEN_DIM))\n",
    "input = jax.random.normal(rng, (BATCH_SIZE, SEQ_LEN, HIDDEN_DIM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(fn, params, input, num_runs=5):\n",
    "  total_time = timeit.timeit(\n",
    "    lambda: jax.tree.map(lambda x: x.block_until_ready(), fn(params, input)),\n",
    "    number=num_runs\n",
    "  )\n",
    "  avg_time = total_time / num_runs\n",
    "  if avg_time < 1e-3:\n",
    "    return f\"{avg_time * 1e6:.2f} μs\"\n",
    "  elif avg_time < 1:\n",
    "    return f\"{avg_time * 1e3:.2f} ms\"\n",
    "  else:\n",
    "    return f\"{avg_time:.2f} s\"\n",
    "\n",
    "def compare_time(fn, params, input, num_runs=5):\n",
    "  assert not isinstance(fn, jaxlib._jax.PjitFunction), \"Function appears to be jitted\"\n",
    "  jit_fn = jax.jit(fn)\n",
    "  # print(f\"Average time for the regular apply: {measure_time(fn, params, input, num_runs)}\")\n",
    "  # print(f\"Average time for the first JIT call: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "  output = jit_fn(params, input)\n",
    "  print(f\"Average time for the subsequent JIT calls: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "\n",
    "def print_cost_analysis(cost, show_utilization=False):\n",
    "    \"\"\"\n",
    "    Pretty-print the cost analysis dictionary returned by:\n",
    "        jax.jit(fn).lower(...).compile().cost_analysis()\n",
    "    \n",
    "    Args:\n",
    "        cost: Dictionary containing cost analysis metrics\n",
    "        show_utilization: Whether to print per-layer utilization metrics\n",
    "    \"\"\"\n",
    "    print(\"=== JAX Cost Analysis ===\")\n",
    "\n",
    "    # Total memory accessed\n",
    "    total_mem = cost.get('bytes accessedout{}', 0)\n",
    "    print(f\"Total estimated memory accessed: {total_mem / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    # Find peak memory estimate across all ops\n",
    "    peak_mem = max(\n",
    "        val for key, val in cost.items() if key.startswith('bytes accessedout{')\n",
    "    )\n",
    "    print(f\"Estimated peak memory usage: {peak_mem / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    # Total FLOPs\n",
    "    flops = cost.get('flops', 0)\n",
    "    print(f\"Estimated FLOPs: {flops / 1e9:.3f} GFLOPs\")\n",
    "\n",
    "    # Transcendental operations\n",
    "    trans = cost.get('transcendentals', 0)\n",
    "    print(f\"Transcendental ops (exp, sin, etc.): {int(trans)}\")\n",
    "\n",
    "    # Optimal execution time (TPU internal estimate)\n",
    "    optimal_time = cost.get('optimal_seconds', None)\n",
    "    if optimal_time is not None:\n",
    "        print(f\"Estimated optimal execution time: {optimal_time * 1e3:.3f} ms\")\n",
    "\n",
    "    if show_utilization:\n",
    "        print(\"=== Per-Layer Utilization (if available) ===\")\n",
    "        utilizations = {\n",
    "            k: v for k, v in cost.items() if k.startswith('utilization')\n",
    "        }\n",
    "\n",
    "        if utilizations:\n",
    "            for key, value in sorted(utilizations.items()):\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"No detailed utilization metrics found.\")\n",
    "    print(); print()\n",
    "            \n",
    "\n",
    "def pyloop_to_scan(params_pyloop):\n",
    "  \"\"\"\n",
    "  Converts a PyLoop-style LLM checkpoint into a scanned-loop style.\n",
    "  \n",
    "  Args:\n",
    "    params_pyloop: Original parameter dict with 'layer_0', 'layer_1', etc.\n",
    "\n",
    "  Returns:\n",
    "    params_scan: Updated dict with all layers stacked under 'layers'.\n",
    "  \"\"\"\n",
    "  t = core.unfreeze(core.FrozenDict(params_pyloop))\n",
    "\n",
    "  # Collect all layer keys like 'layer_0', 'layer_1', etc.\n",
    "  layer_keys = [k for k in t if k.startswith(\"layer_\")]\n",
    "  layer_keys.sort(key=lambda x: int(x.split(\"_\")[1]))  # sort by layer index\n",
    "\n",
    "  depth = len(layer_keys)\n",
    "  assert depth > 0, \"No layers found in params\"\n",
    "\n",
    "  def stack(*values):\n",
    "    return jnp.stack(values)\n",
    "\n",
    "  # Stack all layers under one key: 'layers'\n",
    "  t[\"blocks\"] = jax.tree.map(stack, *[t.pop(k) for k in layer_keys])\n",
    "\n",
    "  return core.freeze(t)\n",
    "\n",
    "def output_comparison(original_output, scan_output, threshold=1e-6, seed=42):\n",
    "    \"\"\"Compare outputs between original and scanned models with detailed formatting.\"\"\"\n",
    "    # Input validation\n",
    "    assert len(original_output) == 2 and len(scan_output) == 2, \"Output must be a tuple of two elements\"\n",
    "    assert original_output[0].shape == scan_output[0].shape, \"Output shapes must match\"\n",
    "    \n",
    "    # Final output comparison\n",
    "    original_last = original_output[0]\n",
    "    scan_last = scan_output[0]\n",
    "    diff = jnp.abs(original_last - scan_last)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'Max diff': float(diff.max()),\n",
    "        'Min diff': float(diff.min()),\n",
    "        'Mean diff': float(diff.mean()),\n",
    "        'Std diff': float(diff.std()),\n",
    "        'Within threshold': float((diff < threshold).mean() * 100)\n",
    "    }\n",
    "    \n",
    "    # Formatting constants\n",
    "    SEP = \"=\" * 80\n",
    "    ROW = \"-\" * 80\n",
    "    COL1 = 20\n",
    "    COL2 = 20\n",
    "    COL3 = 20\n",
    "    \n",
    "    # Print final comparison header\n",
    "    print(f\"\\n{SEP}\")\n",
    "    print(f\"{' FINAL OUTPUT COMPARISON ':.^{len(SEP)}}\")\n",
    "    print(SEP)\n",
    "    print(f\"{'Metric':<{COL1}} | {'Value':>{COL2}} | {'Threshold':>{COL3}}\")\n",
    "    print(ROW)\n",
    "    \n",
    "    # Print final comparison rows\n",
    "    for metric, value in summary.items():\n",
    "        if metric == 'Within threshold':\n",
    "            print(f\"{metric:<{COL1}} | {f'{value:.2f}%':>{COL2}} | {'':>{COL3}}\")\n",
    "        else:\n",
    "            status = \"✅\" if value < threshold else \"❌\"\n",
    "            print(f\"{metric:<{COL1}} | {value:>{COL2}.6f} | {status:>{COL3}}\")\n",
    "    print(f\"{SEP}\\n\")\n",
    "\n",
    "    # Layer-wise comparison\n",
    "    for lyr in range(len(original_output[1])):\n",
    "        lyr_key = f\"layer_{lyr}\"\n",
    "        \n",
    "        # Layer header\n",
    "        print(f\"\\n{SEP}\")\n",
    "        print(f\"{f' LAYER {lyr} COMPARISON ':.^{len(SEP)}}\")\n",
    "        print(SEP)\n",
    "        \n",
    "        for key in original_output[1][lyr_key].keys():\n",
    "            orig_val = original_output[1][lyr_key][key]\n",
    "            scan_val = scan_output[1][key][lyr]\n",
    "            diff = jnp.abs(orig_val - scan_val)\n",
    "            \n",
    "            # Key header\n",
    "            print(f\"\\n{key.upper()}:\")\n",
    "            print(ROW)\n",
    "            print(f\"{'Sampled idx':<5} | {'Original':>15}   | {'Scan':>15} | {'Diff':>15} | {'Status':>5}\")\n",
    "            print(ROW)\n",
    "            \n",
    "            # Random samples\n",
    "            rng = np.random.default_rng(seed)\n",
    "            batch_idx = rng.integers(0, orig_val.shape[0])\n",
    "            seq_idx = rng.integers(0, orig_val.shape[1])\n",
    "            dims = rng.choice(orig_val.shape[2], 3, False)\n",
    "            \n",
    "            for dim in dims:\n",
    "                status = \"✅\" if diff[batch_idx, seq_idx, dim] < threshold else \"❌\"\n",
    "                print(f\"{dim:<5}       | {orig_val[batch_idx, seq_idx, dim]:>15.6f}   | \"\n",
    "                      f\"{scan_val[batch_idx, seq_idx, dim]:>15.6f} | \"\n",
    "                      f\"{diff[batch_idx, seq_idx, dim]:>15.6f} | {status:>5}\")\n",
    "            \n",
    "            # Statistics footer\n",
    "            print(ROW)\n",
    "            print(f\"{'Stats:':<5}      | {'Mean:':>15}   | {'':>15} | {float(diff.mean()):>15.6f} | {'':>5}\")\n",
    "            print(f\"{'':<5}       | {'Std:':>15}   | {'':>15} | {float(diff.std()):>15.6f} | {'':>5}\")\n",
    "            print(f\"{'':<5}       | {'Within threshold:':>15} | {'':>15} | {f'{(diff < threshold).mean() * 100:.2f}%':>15} | {'✅' if (diff < threshold).mean() > 0.99 else '❌':>5}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic iterative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'layer_0': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_1': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_10': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_11': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_12': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_13': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_14': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_15': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_16': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_17': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_18': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_19': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_2': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_20': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_21': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_22': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_23': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_24': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_25': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_26': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_27': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_28': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_29': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_3': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_30': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_31': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_32': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_33': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_34': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_35': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_36': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_37': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_38': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_39': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_4': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_40': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_41': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_5': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_6': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_7': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_8': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_9': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask=None):\n",
    "        out = {}\n",
    "        out[\"input_hidden_state\"] = x\n",
    "        x = out[\"pre_attention_norm\"] = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        # jax.debug.print(\"pre_attention_norm dtype={dtype}\", dtype=x.dtype)\n",
    "        x += nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "\t\t\t\t\t)(x, attention_mask)\n",
    "        # jax.debug.print(\"self_attention dtype={dtype}\", dtype=x.dtype)\n",
    "        out[\"+self_attention\"] = x\n",
    "        x = out[\"pre_ffw_norm\"] = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        # jax.debug.print(\"pre_ffw_norm dtype={dtype}\", dtype=x.dtype)\n",
    "        x += nn.Dense(features=self.hidden_dim)(x)\n",
    "        # jax.debug.print(\"+dense dtype={dtype}\", dtype=x.dtype)\n",
    "        out[\"+dense\"] = x\n",
    "        return x, out\n",
    "\n",
    "class MyIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [\n",
    "            SimpleBlock(\n",
    "                name=f\"layer_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "\t\t\t\t\t\t) \n",
    "            for i in range(self.num_layers)  # Use num_layers instead of hardcoded 3\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"layer_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n",
    "model = MyIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "params = model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different scan configurations for nn.scan\n",
    "scan_args_list = [\n",
    "    # Configuration 1: \n",
    "    # - No parameter axes (shared parameters across layers)\n",
    "    # - Broadcast parameters to all layers\n",
    "    # - Don't split RNGs for parameters\n",
    "    # - Input/Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {},  # No parameter axes (shared params)\n",
    "        \"variable_broadcast\": \"params\",  # Broadcast params to all layers\n",
    "        \"split_rngs\": {\"params\": False},  # Don't split RNGs for params\n",
    "        \"in_axes\": 0,  # Input along axis 0 (batch dim)\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "    # Configuration 2:\n",
    "    # - Parameters along axis 0 (separate params per layer)\n",
    "    # - Don't broadcast parameters\n",
    "    # - Split RNGs for parameters\n",
    "    # - Broadcast input to all layers\n",
    "    # - Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {\"params\": 0},  # Parameters along axis 0 (per layer)\n",
    "        \"variable_broadcast\": False,  # Don't broadcast params\n",
    "        \"split_rngs\": {\"params\": True},  # Split RNGs for params\n",
    "        \"in_axes\": nn.broadcast,  # Broadcast input to all layers\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "    {\n",
    "        \"variable_axes\": {\"params\": 0},  \n",
    "        \"variable_broadcast\": False,\n",
    "        \"split_rngs\": {\"params\": False},\n",
    "        \"in_axes\": nn.broadcast,\n",
    "        \"out_axes\": 0,\n",
    "\t\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'blocks': {'Dense_0': {'bias': (42, 512),\n",
       "    'kernel': (42, 512, 512)},\n",
       "   'RMSNorm_0': {'scale': (42, 512)},\n",
       "   'RMSNorm_1': {'scale': (42, 512)},\n",
       "   'self_attention': {'key': {'bias': (42, 4, 128),\n",
       "     'kernel': (42, 512, 4, 128)},\n",
       "    'out': {'bias': (42, 512), 'kernel': (42, 4, 128, 512)},\n",
       "    'query': {'bias': (42, 4, 128), 'kernel': (42, 512, 4, 128)},\n",
       "    'value': {'bias': (42, 4, 128), 'kernel': (42, 512, 4, 128)}}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX = 1\n",
    "class MyScanModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "      self.blocks = nn.scan(\n",
    "         SimpleBlock,\n",
    "         length=self.num_layers,\n",
    "         **scan_args_list[IDX],\n",
    "\t\t\t)(\n",
    "\t\t\t\thidden_dim=self.hidden_dim,\n",
    "\t\t\t\tnum_heads=self.num_heads,\n",
    "\t\t\t\tdtype=self.dtype\n",
    "\t\t\t)\n",
    "\n",
    "    def __call__(self, x):\n",
    "      x, scan_out = self.blocks(x)\n",
    "      return x, scan_out\n",
    "\n",
    "scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "scan_params = scan_model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, scan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSNorm_0 scale: 0.0\n",
      "RMSNorm_1 scale: 0.0\n",
      "self_attention key kernel: 0.05101369693875313\n",
      "self_attention key bias: 0.0\n",
      "self_attention query kernel: 0.051042985171079636\n",
      "self_attention query bias: 0.0\n",
      "self_attention value kernel: 0.05101339519023895\n",
      "self_attention value bias: 0.0\n",
      "self_attention out kernel: 0.05103050172328949\n",
      "self_attention out bias: 0.0\n",
      "Dense_0 kernel: 0.050579819828271866\n",
      "Dense_0 bias: 0.0\n",
      "RMSNorm_0 scale layer 0: 0.0\n",
      "RMSNorm_1 scale layer 0: 0.0\n",
      "self_attention key kernel layer 0: 0.0\n",
      "self_attention key bias layer 0: 0.0\n",
      "self_attention query kernel layer 0: 0.0\n",
      "self_attention query bias layer 0: 0.0\n",
      "self_attention value kernel layer 0: 0.0\n",
      "self_attention value bias layer 0: 0.0\n",
      "self_attention out kernel layer 0: 0.0\n",
      "self_attention out bias layer 0: 0.0\n",
      "Dense_0 kernel layer 0: 0.0\n",
      "Dense_0 bias layer 0: 0.0\n"
     ]
    }
   ],
   "source": [
    "# pyloop_to_scan\n",
    "new_params = {\"params\": pyloop_to_scan(params['params'])}\n",
    "\n",
    "# compare new_params and scan_params\n",
    "print(f\"RMSNorm_0 scale: {abs(new_params['params']['blocks']['RMSNorm_0']['scale'] - scan_params['params']['blocks']['RMSNorm_0']['scale']).mean()}\")\n",
    "print(f\"RMSNorm_1 scale: {abs(new_params['params']['blocks']['RMSNorm_1']['scale'] - scan_params['params']['blocks']['RMSNorm_1']['scale']).mean()}\")\n",
    "print(f\"self_attention key kernel: {abs(new_params['params']['blocks']['self_attention']['key']['kernel'] - scan_params['params']['blocks']['self_attention']['key']['kernel']).mean()}\")\n",
    "print(f\"self_attention key bias: {abs(new_params['params']['blocks']['self_attention']['key']['bias'] - scan_params['params']['blocks']['self_attention']['key']['bias']).mean()}\")\n",
    "print(f\"self_attention query kernel: {abs(new_params['params']['blocks']['self_attention']['query']['kernel'] - scan_params['params']['blocks']['self_attention']['query']['kernel']).mean()}\")\n",
    "print(f\"self_attention query bias: {abs(new_params['params']['blocks']['self_attention']['query']['bias'] - scan_params['params']['blocks']['self_attention']['query']['bias']).mean()}\")\n",
    "print(f\"self_attention value kernel: {abs(new_params['params']['blocks']['self_attention']['value']['kernel'] - scan_params['params']['blocks']['self_attention']['value']['kernel']).mean()}\")\n",
    "print(f\"self_attention value bias: {abs(new_params['params']['blocks']['self_attention']['value']['bias'] - scan_params['params']['blocks']['self_attention']['value']['bias']).mean()}\")\n",
    "print(f\"self_attention out kernel: {abs(new_params['params']['blocks']['self_attention']['out']['kernel'] - scan_params['params']['blocks']['self_attention']['out']['kernel']).mean()}\")\n",
    "print(f\"self_attention out bias: {abs(new_params['params']['blocks']['self_attention']['out']['bias'] - scan_params['params']['blocks']['self_attention']['out']['bias']).mean()}\")\n",
    "print(f\"Dense_0 kernel: {abs(new_params['params']['blocks']['Dense_0']['kernel'] - scan_params['params']['blocks']['Dense_0']['kernel']).mean()}\")\n",
    "print(f\"Dense_0 bias: {abs(new_params['params']['blocks']['Dense_0']['bias'] - scan_params['params']['blocks']['Dense_0']['bias']).mean()}\")\n",
    "\n",
    "# compare params and new_params\n",
    "print(f\"RMSNorm_0 scale layer 0: {abs(params['params']['layer_0']['RMSNorm_0']['scale'] - new_params['params']['blocks']['RMSNorm_0']['scale'][0]).mean()}\")\n",
    "print(f\"RMSNorm_1 scale layer 0: {abs(params['params']['layer_0']['RMSNorm_1']['scale'] - new_params['params']['blocks']['RMSNorm_1']['scale'][0]).mean()}\")\n",
    "print(f\"self_attention key kernel layer 0: {abs(params['params']['layer_0']['self_attention']['key']['kernel'] - new_params['params']['blocks']['self_attention']['key']['kernel'][0]).mean()}\")\n",
    "print(f\"self_attention key bias layer 0: {abs(params['params']['layer_0']['self_attention']['key']['bias'] - new_params['params']['blocks']['self_attention']['key']['bias'][0]).mean()}\")\n",
    "print(f\"self_attention query kernel layer 0: {abs(params['params']['layer_0']['self_attention']['query']['kernel'] - new_params['params']['blocks']['self_attention']['query']['kernel'][0]).mean()}\")\n",
    "print(f\"self_attention query bias layer 0: {abs(params['params']['layer_0']['self_attention']['query']['bias'] - new_params['params']['blocks']['self_attention']['query']['bias'][0]).mean()}\")\n",
    "print(f\"self_attention value kernel layer 0: {abs(params['params']['layer_0']['self_attention']['value']['kernel'] - new_params['params']['blocks']['self_attention']['value']['kernel'][0]).mean()}\")\n",
    "print(f\"self_attention value bias layer 0: {abs(params['params']['layer_0']['self_attention']['value']['bias'] - new_params['params']['blocks']['self_attention']['value']['bias'][0]).mean()}\")\n",
    "print(f\"self_attention out kernel layer 0: {abs(params['params']['layer_0']['self_attention']['out']['kernel'] - new_params['params']['blocks']['self_attention']['out']['kernel'][0]).mean()}\")\n",
    "print(f\"self_attention out bias layer 0: {abs(params['params']['layer_0']['self_attention']['out']['bias'] - new_params['params']['blocks']['self_attention']['out']['bias'][0]).mean()}\")\n",
    "print(f\"Dense_0 kernel layer 0: {abs(params['params']['layer_0']['Dense_0']['kernel'] - new_params['params']['blocks']['Dense_0']['kernel'][0]).mean()}\")\n",
    "print(f\"Dense_0 bias layer 0: {abs(params['params']['layer_0']['Dense_0']['bias'] - new_params['params']['blocks']['Dense_0']['bias'][0]).mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RematIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        block_cls = nn.remat(\n",
    "            SimpleBlock,\n",
    "            prevent_cse=False,\n",
    "            policy=getattr(jax.checkpoint_policies, \"nothing_saveable\"),\n",
    "        )\n",
    "        self.blocks = [\n",
    "            block_cls(\n",
    "                name=f\"layer_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"layer_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n",
    "remat_model = RematIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "remat_params = remat_model.init(init_rng, dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for the subsequent JIT calls: 582.41 ms\n",
      "=== JAX Cost Analysis ===\n",
      "Total estimated memory accessed: 562.46 MB\n",
      "Estimated peak memory usage: 562.46 MB\n",
      "Estimated FLOPs: 43.476 GFLOPs\n",
      "Transcendental ops (exp, sin, etc.): 2096640\n",
      "\n",
      "\n",
      "Average time for the subsequent JIT calls: 424.66 ms\n",
      "=== JAX Cost Analysis ===\n",
      "Total estimated memory accessed: 180.37 MB\n",
      "Estimated peak memory usage: 180.37 MB\n",
      "Estimated FLOPs: 1.035 GFLOPs\n",
      "Transcendental ops (exp, sin, etc.): 49920\n",
      "\n",
      "\n",
      "Average time for the subsequent JIT calls: 543.31 ms\n",
      "=== JAX Cost Analysis ===\n",
      "Total estimated memory accessed: 562.46 MB\n",
      "Estimated peak memory usage: 562.46 MB\n",
      "Estimated FLOPs: 43.476 GFLOPs\n",
      "Transcendental ops (exp, sin, etc.): 2096640\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For original model\n",
    "compare_time(model.apply, params, input)\n",
    "cost = jax.jit(model.apply).lower(params, input).compile().cost_analysis()\n",
    "print_cost_analysis(cost, show_utilization=False)\n",
    "\n",
    "# For scanned model\n",
    "compare_time(scan_model.apply, scan_params, input)\n",
    "scan_cost = jax.jit(scan_model.apply).lower(scan_params, input).compile().cost_analysis()\n",
    "print_cost_analysis(scan_cost, show_utilization=False)\n",
    "\n",
    "# For remat model\n",
    "compare_time(remat_model.apply, remat_params, input)\n",
    "remat_cost = jax.jit(remat_model.apply).lower(remat_params, input).compile().cost_analysis()\n",
    "print_cost_analysis(remat_cost, show_utilization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output consistency check\n",
    "\n",
    "original_output = model.apply(params, input)\n",
    "scan_output = scan_model.apply(scan_params, input)\n",
    "converted_scan_output = scan_model.apply(new_params, input)\n",
    "remat_output = remat_model.apply(remat_params, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "........................... FINAL OUTPUT COMPARISON ............................\n",
      "================================================================================\n",
      "Metric               |                Value |            Threshold\n",
      "--------------------------------------------------------------------------------\n",
      "Max diff             |             0.000000 |                    ✅\n",
      "Min diff             |             0.000000 |                    ✅\n",
      "Mean diff            |             0.000000 |                    ✅\n",
      "Std diff             |             0.000000 |                    ✅\n",
      "Within threshold     |              100.00% |                     \n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      ".............................. LAYER 0 COMPARISON ..............................\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moutput_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremat_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36moutput_comparison\u001b[39m\u001b[34m(original_output, scan_output, threshold, seed)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m original_output[\u001b[32m1\u001b[39m][lyr_key].keys():\n\u001b[32m    150\u001b[39m     orig_val = original_output[\u001b[32m1\u001b[39m][lyr_key][key]\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     scan_val = \u001b[43mscan_output\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[lyr]\n\u001b[32m    152\u001b[39m     diff = jnp.abs(orig_val - scan_val)\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# Key header\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'input_hidden_state'"
     ]
    }
   ],
   "source": [
    "output_comparison(original_output, remat_output, threshold=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare function\n",
    "# output_comparison(original_output, scan_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Distributed LLM inference with KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCausalAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    \n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cache=None, decode=False):\n",
    "        B, T, D = x.shape\n",
    "        H = self.num_heads\n",
    "        DH = D // H\n",
    "\n",
    "        # Project inputs\n",
    "        q = nn.Dense(D, dtype=self.dtype)(x)\n",
    "        k = nn.Dense(D, dtype=self.dtype)(x)\n",
    "        v = nn.Dense(D, dtype=self.dtype)(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(B, T, H, DH).transpose(0, 2, 1, 3)  # (B, H, T, DH)\n",
    "        k = k.reshape(B, T, H, DH).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, T, H, DH).transpose(0, 2, 1, 3)\n",
    "\n",
    "        if decode:\n",
    "            # In generation mode: update cache\n",
    "            assert cache is not None, \"Cache must be provided during decoding\"\n",
    "            index = cache[\"index\"]\n",
    "            one_hot_indices = jax.nn.one_hot(index, T + 1, dtype=jnp.int32)\n",
    "            k = cache[\"k\"].at[:, :, index:index+T].set(k)\n",
    "            v = cache[\"v\"].at[:, :, index:index+T].set(v)\n",
    "            index += T\n",
    "            cache = {\"k\": k, \"v\": v, \"index\": index}\n",
    "            k = k[:, :, :index]\n",
    "            v = v[:, :, :index]\n",
    "\n",
    "        else:\n",
    "            # During prefilling, use full sequence\n",
    "            pass\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = jnp.einsum(\"bhqd,bhkd->bhqk\", q, k) / jnp.sqrt(DH)\n",
    "        attn_weights = jnp.tril(attn_weights)\n",
    "        attn_weights = jax.nn.softmax(attn_weights)\n",
    "        attn_out = jnp.einsum(\"bhqk,bhvd->bhqd\", attn_weights, v)\n",
    "\n",
    "        # Reshape back\n",
    "        out = attn_out.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "        out = nn.Dense(D, dtype=self.dtype)(out)\n",
    "\n",
    "        return out, cache\n",
    "    \n",
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cache=None, decode=False):\n",
    "        # Self-Attention\n",
    "        attn_out, cache = SimpleCausalAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "        )(x, cache=cache, decode=decode)\n",
    "\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feed-forward\n",
    "        x = x + nn.Sequential([\n",
    "            nn.Dense(self.hidden_dim),\n",
    "            nn.gelu,\n",
    "            nn.Dense(self.hidden_dim)\n",
    "        ])(x)\n",
    "\n",
    "        return x, cache\n",
    "    \n",
    "ScannedBlock = nn.scan(\n",
    "    SimpleBlock,\n",
    "    variable_axes={\"params\": 0},\n",
    "    variable_broadcast=False,\n",
    "    split_rngs={\"params\": True},\n",
    "    in_axes=(nn.broadcast, None),  # x broadcasted, cache not scanned\n",
    "    out_axes=(0, None),  # x stacked, cache not stacked\n",
    "    length=LAYERS\n",
    ")\n",
    "\n",
    "class SimpleLLM(nn.Module):\n",
    "    vocab_size: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    max_len: int\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input_ids, cache=None, decode=False):\n",
    "        embed = nn.Embed(num_embeddings=self.vocab_size, features=self.hidden_dim)\n",
    "        x = embed(input_ids)\n",
    "\n",
    "        # Stack blocks\n",
    "        blocks = ScannedBlock(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        x, cache = blocks(x, cache=cache, decode=decode)\n",
    "\n",
    "        # Final output logits\n",
    "        logits = nn.Dense(self.vocab_size)(x)\n",
    "\n",
    "        return logits, cache\n",
    "\n",
    "def init_cache(model, params, batch_size, max_seq_len):\n",
    "    @jax.jit\n",
    "    def _init_cache():\n",
    "        dummy_input = jnp.ones((batch_size, 1), dtype=jnp.int32)\n",
    "        _, initial_cache = model.apply(params, dummy_input, decode=True, mutable=[\"cache\"])\n",
    "        return initial_cache\n",
    "    return _init_cache()\n",
    "\n",
    "def generate_tokens(model, params, tokenizer, prompt, max_new_tokens=30):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "    # JIT once\n",
    "    @jax.jit\n",
    "    def forward_step(input_ids, cache):\n",
    "        logits, new_cache = model.apply(params, input_ids, cache=cache, decode=True)\n",
    "        return logits, new_cache\n",
    "\n",
    "    # Initialize cache\n",
    "    cache = init_cache(model, params, input_ids.shape[0], max_new_tokens)\n",
    "    print(jax.tree.map(jnp.shape, cache))\n",
    "\n",
    "    # Prefill context\n",
    "    logits, cache = forward_step(input_ids, cache)\n",
    "\n",
    "    # Generate new tokens\n",
    "    generated_ids = []\n",
    "    current_id = input_ids[:, -1:]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, cache = forward_step(current_id, cache)\n",
    "        current_id = jnp.argmax(logits[:, -1:], axis=-1)\n",
    "        generated_ids.append(current_id)\n",
    "\n",
    "    return tokenizer.decode(jnp.concatenate(generated_ids, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/flax/core/lift.py:310: RuntimeWarning: kwargs are not supported in scan, so \"cache, decode\" is(are) ignored\n",
      "  warnings.warn(msg.format(name, ', '.join(kwargs.keys())), RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tuple arity mismatch: 0 != 2; tuple: ().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m DTYPE = jnp.float32\n\u001b[32m      8\u001b[39m model = SimpleLLM(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_len=MAX_LEN, dtype=DTYPE)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m params = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m jax.tree.map(jnp.shape, params)\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mSimpleLLM.__call__\u001b[39m\u001b[34m(self, input_ids, cache, decode)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Stack blocks\u001b[39;00m\n\u001b[32m     99\u001b[39m blocks = ScannedBlock(\n\u001b[32m    100\u001b[39m     hidden_dim=\u001b[38;5;28mself\u001b[39m.hidden_dim,\n\u001b[32m    101\u001b[39m     num_heads=\u001b[38;5;28mself\u001b[39m.num_heads,\n\u001b[32m    102\u001b[39m     dtype=\u001b[38;5;28mself\u001b[39m.dtype\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m x, cache = \u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Final output logits\u001b[39;00m\n\u001b[32m    107\u001b[39m logits = nn.Dense(\u001b[38;5;28mself\u001b[39m.vocab_size)(x)\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/jax/_src/tree_util.py:360\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.tree.map`.\"\"\"\u001b[39;00m\n\u001b[32m    359\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m all_leaves = [leaves] + [\u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(f(*xs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
      "\u001b[31mValueError\u001b[39m: Tuple arity mismatch: 0 != 2; tuple: ()."
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 42\n",
    "NUM_HEADS = 4\n",
    "MAX_LEN = 1024\n",
    "DTYPE = jnp.float32\n",
    "\n",
    "model = SimpleLLM(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_len=MAX_LEN, dtype=DTYPE)\n",
    "params = model.init(init_rng, jnp.ones((1, 32), dtype=jnp.int32))\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
