{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flax.linen.scan module can be used to reduce the whole repeated or iterative computation graph into a single interation, therefore reduces the graph memory and compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules so we can edit the code and have it automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import timeit\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def measure_time(fn, params, input, num_runs=5):\n",
    "  total_time = timeit.timeit(\n",
    "    lambda: jax.tree.map(lambda x: x.block_until_ready(), fn(params, input)),\n",
    "    number=num_runs\n",
    "  )\n",
    "  avg_time = total_time / num_runs\n",
    "  if avg_time < 1e-3:\n",
    "    return f\"{avg_time * 1e6:.2f} Î¼s\"\n",
    "  elif avg_time < 1:\n",
    "    return f\"{avg_time * 1e3:.2f} ms\"\n",
    "  else:\n",
    "    return f\"{avg_time:.2f} s\"\n",
    "\n",
    "def compare_time(fn, params, input, num_runs=5):\n",
    "  assert not isinstance(fn, jaxlib._jax.PjitFunction), \"Function appears to be jitted\"\n",
    "  jit_fn = jax.jit(fn)\n",
    "  print(f\"Time for the regular apply: {measure_time(fn, params, input, num_runs)}\")\n",
    "  print(f\"Time for the first JIT call: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "  print(f\"Time for the subsequent JIT calls: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 32\n",
    "HIDDEN_DIM = 1024\n",
    "LAYERS = 42\n",
    "NUM_HEADS = 4\n",
    "DTYPE = jnp.float32\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "dummy_input = jax.random.normal(rng, (1, SEQ_LEN, HIDDEN_DIM))\n",
    "input = jax.random.normal(rng, (BATCH_SIZE, SEQ_LEN, HIDDEN_DIM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic iterative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for the regular apply: 1.07 s\n",
      "Time for the first JIT call: 1.23 s\n",
      "Time for the subsequent JIT calls: 2.91 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'layer_0': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_1': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_10': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_11': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_12': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_13': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_14': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_15': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_16': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_17': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_18': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_19': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_2': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_20': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_21': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_22': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_23': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_24': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_25': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_26': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_27': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_28': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_29': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_3': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_30': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_31': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_32': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_33': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_34': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_35': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_36': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_37': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_38': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_39': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_4': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_40': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_41': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_5': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_6': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_7': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_8': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'layer_9': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask=None):\n",
    "        out = {}\n",
    "        out[\"input_hidden_state\"] = x\n",
    "        x += nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "\t\t\t\t\t)(x, attention_mask)\n",
    "        out[\"+self_attention\"] = x\n",
    "        x += nn.Dense(features=self.hidden_dim)(x)\n",
    "        out[\"+dense\"] = x\n",
    "        return x, out\n",
    "\n",
    "class MyIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [\n",
    "            SimpleBlock(\n",
    "                name=f\"layer_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "\t\t\t\t\t\t) \n",
    "            for i in range(self.num_layers)  # Use num_layers instead of hardcoded 3\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"layer_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n",
    "model = MyIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "params = model.init(init_rng, dummy_input)\n",
    "\n",
    "compare_time(model.apply, params, input)\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different scan configurations for nn.scan\n",
    "scan_args_list = [\n",
    "    # Configuration 1: \n",
    "    # - No parameter axes (shared parameters across layers)\n",
    "    # - Broadcast parameters to all layers\n",
    "    # - Don't split RNGs for parameters\n",
    "    # - Input/Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {},  # No parameter axes (shared params)\n",
    "        \"variable_broadcast\": \"params\",  # Broadcast params to all layers\n",
    "        \"split_rngs\": {\"params\": False},  # Don't split RNGs for params\n",
    "        \"in_axes\": 0,  # Input along axis 0 (batch dim)\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "    # Configuration 2:\n",
    "    # - Parameters along axis 0 (separate params per layer)\n",
    "    # - Don't broadcast parameters\n",
    "    # - Split RNGs for parameters\n",
    "    # - Broadcast input to all layers\n",
    "    # - Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {\"params\": 0},  # Parameters along axis 0 (per layer)\n",
    "        \"variable_broadcast\": False,  # Don't broadcast params\n",
    "        \"split_rngs\": {\"params\": True},  # Split RNGs for params\n",
    "        \"in_axes\": nn.broadcast,  # Broadcast input to all layers\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for the regular apply: 532.56 ms\n",
      "Time for the first JIT call: 95.10 ms\n",
      "Time for the subsequent JIT calls: 3.49 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'blocks': {'Dense_0': {'bias': (42, 1024),\n",
       "    'kernel': (42, 1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (42, 4, 256),\n",
       "     'kernel': (42, 1024, 4, 256)},\n",
       "    'out': {'bias': (42, 1024), 'kernel': (42, 4, 256, 1024)},\n",
       "    'query': {'bias': (42, 4, 256), 'kernel': (42, 1024, 4, 256)},\n",
       "    'value': {'bias': (42, 4, 256), 'kernel': (42, 1024, 4, 256)}}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX = 1\n",
    "class MyScanModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "      self.blocks = nn.scan(\n",
    "         SimpleBlock,\n",
    "         length=self.num_layers,\n",
    "         **scan_args_list[IDX],\n",
    "\t\t\t)(\n",
    "\t\t\t\thidden_dim=self.hidden_dim,\n",
    "\t\t\t\tnum_heads=self.num_heads,\n",
    "\t\t\t\tdtype=self.dtype\n",
    "\t\t\t)\n",
    "\n",
    "    def __call__(self, x):\n",
    "      x, scan_out = self.blocks(x)\n",
    "      return x, scan_out\n",
    "\n",
    "scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "scan_params = scan_model.init(init_rng, dummy_input)\n",
    "\n",
    "compare_time(scan_model.apply, scan_params, input)\n",
    "\n",
    "jax.tree.map(jnp.shape, scan_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Distributed LLM inference with KV cachingm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
