{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flax.linen.scan module can be used to reduce the whole repeated or iterative computation graph into a single interation, therefore reduces the graph memory and compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules so we can edit the code and have it automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "SEQ_LEN = 32\n",
    "HIDDEN_DIM = 512\n",
    "LAYERS = 3\n",
    "NUM_HEADS = 4\n",
    "DTYPE = jnp.float32\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "dummy_input = jax.random.normal(rng, (1, SEQ_LEN, HIDDEN_DIM))\n",
    "input = jax.random.normal(rng, (BATCH_SIZE, SEQ_LEN, HIDDEN_DIM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(fn, params, input, num_runs=5):\n",
    "  total_time = timeit.timeit(\n",
    "    lambda: jax.tree.map(lambda x: x.block_until_ready(), fn(params, input)),\n",
    "    number=num_runs\n",
    "  )\n",
    "  avg_time = total_time / num_runs\n",
    "  if avg_time < 1e-3:\n",
    "    return f\"{avg_time * 1e6:.2f} μs\"\n",
    "  elif avg_time < 1:\n",
    "    return f\"{avg_time * 1e3:.2f} ms\"\n",
    "  else:\n",
    "    return f\"{avg_time:.2f} s\"\n",
    "\n",
    "def compare_time(fn, params, input, num_runs=5):\n",
    "  assert not isinstance(fn, jaxlib._jax.PjitFunction), \"Function appears to be jitted\"\n",
    "  jit_fn = jax.jit(fn)\n",
    "  # print(f\"Average time for the regular apply: {measure_time(fn, params, input, num_runs)}\")\n",
    "  # print(f\"Average time for the first JIT call: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "  output = jit_fn(params, input)\n",
    "  print(f\"Average time for the subsequent JIT calls: {measure_time(jit_fn, params, input, num_runs)}\")\n",
    "\n",
    "def print_cost_analysis(cost, show_utilization=False):\n",
    "    \"\"\"\n",
    "    Pretty-print the cost analysis dictionary returned by:\n",
    "        jax.jit(fn).lower(...).compile().cost_analysis()\n",
    "    \n",
    "    Args:\n",
    "        cost: Dictionary containing cost analysis metrics\n",
    "        show_utilization: Whether to print per-layer utilization metrics\n",
    "    \"\"\"\n",
    "    print(\"=== JAX Cost Analysis ===\")\n",
    "\n",
    "    # Total memory accessed\n",
    "    total_mem = cost.get('bytes accessedout{}', 0)\n",
    "    print(f\"Total estimated memory accessed: {total_mem / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    # Find peak memory estimate across all ops\n",
    "    peak_mem = max(\n",
    "        val for key, val in cost.items() if key.startswith('bytes accessedout{')\n",
    "    )\n",
    "    print(f\"Estimated peak memory usage: {peak_mem / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    # Total FLOPs\n",
    "    flops = cost.get('flops', 0)\n",
    "    print(f\"Estimated FLOPs: {flops / 1e9:.3f} GFLOPs\")\n",
    "\n",
    "    # Transcendental operations\n",
    "    trans = cost.get('transcendentals', 0)\n",
    "    print(f\"Transcendental ops (exp, sin, etc.): {int(trans)}\")\n",
    "\n",
    "    # Optimal execution time (TPU internal estimate)\n",
    "    optimal_time = cost.get('optimal_seconds', None)\n",
    "    if optimal_time is not None:\n",
    "        print(f\"Estimated optimal execution time: {optimal_time * 1e3:.3f} ms\")\n",
    "\n",
    "    if show_utilization:\n",
    "        print(\"=== Per-Layer Utilization (if available) ===\")\n",
    "        utilizations = {\n",
    "            k: v for k, v in cost.items() if k.startswith('utilization')\n",
    "        }\n",
    "\n",
    "        if utilizations:\n",
    "            for key, value in sorted(utilizations.items()):\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"No detailed utilization metrics found.\")\n",
    "    print(); print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic iterative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'layer_0': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_1': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}},\n",
       "  'layer_2': {'Dense_0': {'bias': (512,), 'kernel': (512, 512)},\n",
       "   'RMSNorm_0': {'scale': (512,)},\n",
       "   'RMSNorm_1': {'scale': (512,)},\n",
       "   'self_attention': {'key': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'out': {'bias': (512,), 'kernel': (4, 128, 512)},\n",
       "    'query': {'bias': (4, 128), 'kernel': (512, 4, 128)},\n",
       "    'value': {'bias': (4, 128), 'kernel': (512, 4, 128)}}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask=None):\n",
    "        out = {}\n",
    "        out[\"input_hidden_state\"] = x\n",
    "        x = out[\"pre_attention_norm\"] = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        x += nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "\t\t\t\t\t)(x, attention_mask)\n",
    "        out[\"+self_attention\"] = x\n",
    "        x = out[\"pre_ffw_norm\"] = nn.RMSNorm(dtype=self.dtype)(x)\n",
    "        x += nn.Dense(features=self.hidden_dim)(x)\n",
    "        out[\"+dense\"] = x\n",
    "        return x, out\n",
    "\n",
    "class MyIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [\n",
    "            SimpleBlock(\n",
    "                name=f\"layer_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "\t\t\t\t\t\t) \n",
    "            for i in range(self.num_layers)  # Use num_layers instead of hardcoded 3\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"layer_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n",
    "model = MyIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "params = model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different scan configurations for nn.scan\n",
    "scan_args_list = [\n",
    "    # Configuration 1: \n",
    "    # - No parameter axes (shared parameters across layers)\n",
    "    # - Broadcast parameters to all layers\n",
    "    # - Don't split RNGs for parameters\n",
    "    # - Input/Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {},  # No parameter axes (shared params)\n",
    "        \"variable_broadcast\": \"params\",  # Broadcast params to all layers\n",
    "        \"split_rngs\": {\"params\": False},  # Don't split RNGs for params\n",
    "        \"in_axes\": 0,  # Input along axis 0 (batch dim)\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "    # Configuration 2:\n",
    "    # - Parameters along axis 0 (separate params per layer)\n",
    "    # - Don't broadcast parameters\n",
    "    # - Split RNGs for parameters\n",
    "    # - Broadcast input to all layers\n",
    "    # - Output along axis 0 (batch dimension)\n",
    "    {\n",
    "        \"variable_axes\": {\"params\": 0},  # Parameters along axis 0 (per layer)\n",
    "        \"variable_broadcast\": False,  # Don't broadcast params\n",
    "        \"split_rngs\": {\"params\": True},  # Split RNGs for params\n",
    "        \"in_axes\": nn.broadcast,  # Broadcast input to all layers\n",
    "        \"out_axes\": 0,  # Output along axis 0 (batch dim)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'blocks': {'Dense_0': {'bias': (3, 512), 'kernel': (3, 512, 512)},\n",
       "   'RMSNorm_0': {'scale': (3, 512)},\n",
       "   'RMSNorm_1': {'scale': (3, 512)},\n",
       "   'self_attention': {'key': {'bias': (3, 4, 128), 'kernel': (3, 512, 4, 128)},\n",
       "    'out': {'bias': (3, 512), 'kernel': (3, 4, 128, 512)},\n",
       "    'query': {'bias': (3, 4, 128), 'kernel': (3, 512, 4, 128)},\n",
       "    'value': {'bias': (3, 4, 128), 'kernel': (3, 512, 4, 128)}}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX = 1\n",
    "class MyScanModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "      self.blocks = nn.scan(\n",
    "         SimpleBlock,\n",
    "         length=self.num_layers,\n",
    "         **scan_args_list[IDX],\n",
    "\t\t\t)(\n",
    "\t\t\t\thidden_dim=self.hidden_dim,\n",
    "\t\t\t\tnum_heads=self.num_heads,\n",
    "\t\t\t\tdtype=self.dtype\n",
    "\t\t\t)\n",
    "\n",
    "    def __call__(self, x):\n",
    "      x, scan_out = self.blocks(x)\n",
    "      return x, scan_out\n",
    "\n",
    "scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "scan_params = scan_model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, scan_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for the subsequent JIT calls: 1.18 ms\n",
      "=== JAX Cost Analysis ===\n",
      "Total estimated memory accessed: 66.72 MB\n",
      "Estimated peak memory usage: 66.72 MB\n",
      "Estimated FLOPs: 3.109 GFLOPs\n",
      "Transcendental ops (exp, sin, etc.): 149760\n",
      "Estimated optimal execution time: -13999.835 ms\n",
      "\n",
      "\n",
      "Average time for the subsequent JIT calls: 1.40 ms\n",
      "=== JAX Cost Analysis ===\n",
      "Total estimated memory accessed: 42.83 MB\n",
      "Estimated peak memory usage: 42.83 MB\n",
      "Estimated FLOPs: 1.042 GFLOPs\n",
      "Transcendental ops (exp, sin, etc.): 49920\n",
      "Estimated optimal execution time: -3999.909 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For original model\n",
    "compare_time(model.apply, params, input)\n",
    "cost = jax.jit(model.apply).lower(params, input).compile().cost_analysis()\n",
    "print_cost_analysis(cost, show_utilization=False)\n",
    "\n",
    "# For scanned model\n",
    "compare_time(scan_model.apply, scan_params, input)\n",
    "scan_cost = jax.jit(scan_model.apply).lower(scan_params, input).compile().cost_analysis()\n",
    "print_cost_analysis(scan_cost, show_utilization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 32, 512),\n",
       " {'layer_0': {'+dense': (12, 32, 512),\n",
       "   '+self_attention': (12, 32, 512),\n",
       "   'input_hidden_state': (12, 32, 512),\n",
       "   'pre_attention_norm': (12, 32, 512),\n",
       "   'pre_ffw_norm': (12, 32, 512)},\n",
       "  'layer_1': {'+dense': (12, 32, 512),\n",
       "   '+self_attention': (12, 32, 512),\n",
       "   'input_hidden_state': (12, 32, 512),\n",
       "   'pre_attention_norm': (12, 32, 512),\n",
       "   'pre_ffw_norm': (12, 32, 512)},\n",
       "  'layer_2': {'+dense': (12, 32, 512),\n",
       "   '+self_attention': (12, 32, 512),\n",
       "   'input_hidden_state': (12, 32, 512),\n",
       "   'pre_attention_norm': (12, 32, 512),\n",
       "   'pre_ffw_norm': (12, 32, 512)}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output consistency check\n",
    "\n",
    "original_output = model.apply(params, input)\n",
    "scan_output = scan_model.apply(scan_params, input)\n",
    "\n",
    "jax.tree.map(jnp.shape, original_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_comparison(original_output, scan_output, threshold=1e-10, seed=42):\n",
    "    \"\"\"Compare outputs between original and scanned models with detailed formatting.\"\"\"\n",
    "    # Input validation\n",
    "    assert len(original_output) == 2 and len(scan_output) == 2, \"Output must be a tuple of two elements\"\n",
    "    assert original_output[0].shape == scan_output[0].shape, \"Output shapes must match\"\n",
    "    \n",
    "    # Final output comparison\n",
    "    original_last = original_output[0]\n",
    "    scan_last = scan_output[0]\n",
    "    diff = jnp.abs(original_last - scan_last)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'Max diff': float(diff.max()),\n",
    "        'Min diff': float(diff.min()),\n",
    "        'Mean diff': float(diff.mean()),\n",
    "        'Std diff': float(diff.std()),\n",
    "        'Within threshold': float((diff < threshold).mean() * 100)\n",
    "    }\n",
    "    \n",
    "    # Formatting constants\n",
    "    SEP = \"=\" * 60\n",
    "    ROW = \"-\" * 60\n",
    "    COL1 = 20\n",
    "    COL2 = 20\n",
    "    COL3 = 15\n",
    "    \n",
    "    # Print final comparison header\n",
    "    print(f\"\\n{SEP}\")\n",
    "    print(f\"{' FINAL OUTPUT COMPARISON ':.^{len(SEP)}}\")\n",
    "    print(SEP)\n",
    "    print(f\"{'Metric':<{COL1}} | {'Value':>{COL2}} | {'Threshold':>{COL3}}\")\n",
    "    print(ROW)\n",
    "    \n",
    "    # Print final comparison rows\n",
    "    for metric, value in summary.items():\n",
    "        if metric == 'Within threshold':\n",
    "            print(f\"{metric:<{COL1}} | {f'{value:.2f}%':>{COL2}} | {'':>{COL3}}\")\n",
    "        else:\n",
    "            status = \"✅\" if value < threshold else \"❌\"\n",
    "            print(f\"{metric:<{COL1}} | {value:>{COL2}.6f} | {status:>{COL3}}\")\n",
    "    print(f\"{SEP}\\n\")\n",
    "\n",
    "    # Layer-wise comparison\n",
    "    for lyr in range(len(original_output[1])):\n",
    "        lyr_key = f\"layer_{lyr}\"\n",
    "        \n",
    "        # Layer header\n",
    "        print(f\"\\n{SEP}\")\n",
    "        print(f\"{f' LAYER {lyr} COMPARISON ':.^{len(SEP)}}\")\n",
    "        print(SEP)\n",
    "        \n",
    "        for key in original_output[1][lyr_key].keys():\n",
    "            orig_val = original_output[1][lyr_key][key]\n",
    "            scan_val = scan_output[1][key][lyr]\n",
    "            diff = jnp.abs(orig_val - scan_val)\n",
    "            \n",
    "            # Key header\n",
    "            print(f\"\\n{key.upper()}:\")\n",
    "            print(ROW)\n",
    "            print(f\"{'Sampled idx':<5} | {'Original':>15}   | {'Scan':>15} | {'Diff':>15} | {'Status':>5}\")\n",
    "            print(ROW)\n",
    "            \n",
    "            # Random samples\n",
    "            rng = np.random.default_rng(seed)\n",
    "            batch_idx = rng.integers(0, orig_val.shape[0])\n",
    "            seq_idx = rng.integers(0, orig_val.shape[1])\n",
    "            dims = rng.choice(orig_val.shape[2], 3, False)\n",
    "            \n",
    "            for dim in dims:\n",
    "                status = \"✅\" if diff[batch_idx, seq_idx, dim] < threshold else \"❌\"\n",
    "                print(f\"{dim:<5}       | {orig_val[batch_idx, seq_idx, dim]:>15.6f}   | \"\n",
    "                      f\"{scan_val[batch_idx, seq_idx, dim]:>15.6f} | \"\n",
    "                      f\"{diff[batch_idx, seq_idx, dim]:>15.6f} | {status:>5}\")\n",
    "            \n",
    "            # Statistics footer\n",
    "            print(ROW)\n",
    "            print(f\"{'Stats:':<5}      | {'Mean:':>15}   | {'':>15} | {float(diff.mean()):>15.6f} | {'':>5}\")\n",
    "            print(f\"{'':<5}       | {'Std:':>15}   | {'':>15} | {float(diff.std()):>15.6f} | {'':>5}\")\n",
    "            print(f\"{'':<5}       | {'Within threshold:':>15} | {'':>15} | {f'{(diff < threshold).mean() * 100:.2f}%':>15} | {'✅' if (diff < threshold).mean() > 0.99 else '❌':>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "................. FINAL OUTPUT COMPARISON ..................\n",
      "============================================================\n",
      "Metric               |                Value |       Threshold\n",
      "------------------------------------------------------------\n",
      "Max diff             |             8.758924 |               ❌\n",
      "Min diff             |             0.000001 |               ❌\n",
      "Mean diff            |             1.523460 |               ❌\n",
      "Std diff             |             1.151397 |               ❌\n",
      "Within threshold     |                0.00% |                \n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      ".................... LAYER 0 COMPARISON ....................\n",
      "============================================================\n",
      "\n",
      "INPUT_HIDDEN_STATE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |        1.545792   |        1.545792 |        0.000000 |     ✅\n",
      "333         |       -0.552857   |       -0.552857 |        0.000000 |     ✅\n",
      "221         |        0.066069   |        0.066069 |        0.000000 |     ✅\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.000000 |      \n",
      "            |            Std:   |                 |        0.000000 |      \n",
      "            | Within threshold: |                 |         100.00% |     ✅\n",
      "\n",
      "PRE_ATTENTION_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |        1.481512   |        1.481512 |        0.000000 |     ✅\n",
      "333         |       -0.529867   |       -0.529867 |        0.000000 |     ✅\n",
      "221         |        0.063321   |        0.063321 |        0.000000 |     ✅\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.000000 |      \n",
      "            |            Std:   |                 |        0.000000 |      \n",
      "            | Within threshold: |                 |         100.00% |     ✅\n",
      "\n",
      "+SELF_ATTENTION:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |        1.643823   |        1.510004 |        0.133818 |     ❌\n",
      "333         |       -0.304148   |       -0.505447 |        0.201298 |     ❌\n",
      "221         |        0.104996   |        0.170617 |        0.065621 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.306245 |      \n",
      "            |            Std:   |                 |        0.233453 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "PRE_FFW_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |        1.610525   |        1.470703 |        0.139821 |     ❌\n",
      "333         |       -0.297987   |       -0.492291 |        0.194304 |     ❌\n",
      "221         |        0.102869   |        0.166176 |        0.063307 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.294815 |      \n",
      "            |            Std:   |                 |        0.224499 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "+DENSE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.342755   |        2.110115 |        2.452870 |     ❌\n",
      "333         |       -0.041557   |       -0.313747 |        0.272190 |     ❌\n",
      "221         |       -0.523794   |        0.328805 |        0.852599 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.166800 |      \n",
      "            |            Std:   |                 |        0.880824 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "============================================================\n",
      ".................... LAYER 1 COMPARISON ....................\n",
      "============================================================\n",
      "\n",
      "INPUT_HIDDEN_STATE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.342755   |        2.110115 |        2.452870 |     ❌\n",
      "333         |       -0.041557   |       -0.313747 |        0.272190 |     ❌\n",
      "221         |       -0.523794   |        0.328805 |        0.852599 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.166800 |      \n",
      "            |            Std:   |                 |        0.880824 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "PRE_ATTENTION_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.248662   |        1.509059 |        1.757721 |     ❌\n",
      "333         |       -0.030149   |       -0.224378 |        0.194229 |     ❌\n",
      "221         |       -0.380002   |        0.235146 |        0.615148 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.824521 |      \n",
      "            |            Std:   |                 |        0.622155 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "+SELF_ATTENTION:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.486793   |        1.511619 |        1.998412 |     ❌\n",
      "333         |       -0.601402   |        0.502993 |        1.104394 |     ❌\n",
      "221         |       -0.366735   |       -0.106536 |        0.260199 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.897941 |      \n",
      "            |            Std:   |                 |        0.678070 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "PRE_FFW_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.448746   |        1.452204 |        1.900950 |     ❌\n",
      "333         |       -0.554398   |        0.483222 |        1.037620 |     ❌\n",
      "221         |       -0.338072   |       -0.102348 |        0.235724 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        0.857287 |      \n",
      "            |            Std:   |                 |        0.647326 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "+DENSE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.929368   |        0.637519 |        1.566887 |     ❌\n",
      "333         |        0.293743   |        0.896753 |        0.603010 |     ❌\n",
      "221         |        0.370147   |        1.114164 |        0.744018 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.417254 |      \n",
      "            |            Std:   |                 |        1.070725 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "============================================================\n",
      ".................... LAYER 2 COMPARISON ....................\n",
      "============================================================\n",
      "\n",
      "INPUT_HIDDEN_STATE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.929368   |        0.637519 |        1.566887 |     ❌\n",
      "333         |        0.293743   |        0.896753 |        0.603010 |     ❌\n",
      "221         |        0.370147   |        1.114164 |        0.744018 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.417254 |      \n",
      "            |            Std:   |                 |        1.070725 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "PRE_ATTENTION_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -0.645452   |        0.452135 |        1.097587 |     ❌\n",
      "333         |        0.204007   |        0.635986 |        0.431979 |     ❌\n",
      "221         |        0.257069   |        0.790176 |        0.533107 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.001612 |      \n",
      "            |            Std:   |                 |        0.756212 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "+SELF_ATTENTION:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -1.227766   |        0.403638 |        1.631403 |     ❌\n",
      "333         |        0.140197   |        0.104970 |        0.035227 |     ❌\n",
      "221         |        0.604656   |        0.722394 |        0.117738 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.091892 |      \n",
      "            |            Std:   |                 |        0.824716 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "PRE_FFW_NORM:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -1.155021   |        0.381603 |        1.536624 |     ❌\n",
      "333         |        0.131890   |        0.099240 |        0.032651 |     ❌\n",
      "221         |        0.568830   |        0.682959 |        0.114128 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.019204 |      \n",
      "            |            Std:   |                 |        0.769610 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n",
      "\n",
      "+DENSE:\n",
      "------------------------------------------------------------\n",
      "Sampled idx |        Original   |            Scan |            Diff | Status\n",
      "------------------------------------------------------------\n",
      "224         |       -1.074686   |        1.077024 |        2.151710 |     ❌\n",
      "333         |        0.256474   |        1.949058 |        1.692584 |     ❌\n",
      "221         |        0.827292   |        0.952938 |        0.125647 |     ❌\n",
      "------------------------------------------------------------\n",
      "Stats:      |           Mean:   |                 |        1.523460 |      \n",
      "            |            Std:   |                 |        1.151397 |      \n",
      "            | Within threshold: |                 |           0.00% |     ❌\n"
     ]
    }
   ],
   "source": [
    "# compare function\n",
    "output_comparison(original_output, scan_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Distributed LLM inference with KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCausalAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    \n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cache=None, decode=False):\n",
    "        B, T, D = x.shape\n",
    "        H = self.num_heads\n",
    "        DH = D // H\n",
    "\n",
    "        # Project inputs\n",
    "        q = nn.Dense(D, dtype=self.dtype)(x)\n",
    "        k = nn.Dense(D, dtype=self.dtype)(x)\n",
    "        v = nn.Dense(D, dtype=self.dtype)(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(B, T, H, DH).transpose(0, 2, 1, 3)  # (B, H, T, DH)\n",
    "        k = k.reshape(B, T, H, DH).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(B, T, H, DH).transpose(0, 2, 1, 3)\n",
    "\n",
    "        if decode:\n",
    "            # In generation mode: update cache\n",
    "            assert cache is not None, \"Cache must be provided during decoding\"\n",
    "            index = cache[\"index\"]\n",
    "            one_hot_indices = jax.nn.one_hot(index, T + 1, dtype=jnp.int32)\n",
    "            k = cache[\"k\"].at[:, :, index:index+T].set(k)\n",
    "            v = cache[\"v\"].at[:, :, index:index+T].set(v)\n",
    "            index += T\n",
    "            cache = {\"k\": k, \"v\": v, \"index\": index}\n",
    "            k = k[:, :, :index]\n",
    "            v = v[:, :, :index]\n",
    "\n",
    "        else:\n",
    "            # During prefilling, use full sequence\n",
    "            pass\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = jnp.einsum(\"bhqd,bhkd->bhqk\", q, k) / jnp.sqrt(DH)\n",
    "        attn_weights = jnp.tril(attn_weights)\n",
    "        attn_weights = jax.nn.softmax(attn_weights)\n",
    "        attn_out = jnp.einsum(\"bhqk,bhvd->bhqd\", attn_weights, v)\n",
    "\n",
    "        # Reshape back\n",
    "        out = attn_out.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "        out = nn.Dense(D, dtype=self.dtype)(out)\n",
    "\n",
    "        return out, cache\n",
    "    \n",
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, cache=None, decode=False):\n",
    "        # Self-Attention\n",
    "        attn_out, cache = SimpleCausalAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "        )(x, cache=cache, decode=decode)\n",
    "\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feed-forward\n",
    "        x = x + nn.Sequential([\n",
    "            nn.Dense(self.hidden_dim),\n",
    "            nn.gelu,\n",
    "            nn.Dense(self.hidden_dim)\n",
    "        ])(x)\n",
    "\n",
    "        return x, cache\n",
    "    \n",
    "ScannedBlock = nn.scan(\n",
    "    SimpleBlock,\n",
    "    variable_axes={\"params\": 0},\n",
    "    variable_broadcast=False,\n",
    "    split_rngs={\"params\": True},\n",
    "    in_axes=(nn.broadcast, None),  # x broadcasted, cache not scanned\n",
    "    out_axes=(0, None),  # x stacked, cache not stacked\n",
    "    length=LAYERS\n",
    ")\n",
    "\n",
    "class SimpleLLM(nn.Module):\n",
    "    vocab_size: int\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    max_len: int\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input_ids, cache=None, decode=False):\n",
    "        embed = nn.Embed(num_embeddings=self.vocab_size, features=self.hidden_dim)\n",
    "        x = embed(input_ids)\n",
    "\n",
    "        # Stack blocks\n",
    "        blocks = ScannedBlock(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        x, cache = blocks(x, cache=cache, decode=decode)\n",
    "\n",
    "        # Final output logits\n",
    "        logits = nn.Dense(self.vocab_size)(x)\n",
    "\n",
    "        return logits, cache\n",
    "\n",
    "def init_cache(model, params, batch_size, max_seq_len):\n",
    "    @jax.jit\n",
    "    def _init_cache():\n",
    "        dummy_input = jnp.ones((batch_size, 1), dtype=jnp.int32)\n",
    "        _, initial_cache = model.apply(params, dummy_input, decode=True, mutable=[\"cache\"])\n",
    "        return initial_cache\n",
    "    return _init_cache()\n",
    "\n",
    "def generate_tokens(model, params, tokenizer, prompt, max_new_tokens=30):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "    # JIT once\n",
    "    @jax.jit\n",
    "    def forward_step(input_ids, cache):\n",
    "        logits, new_cache = model.apply(params, input_ids, cache=cache, decode=True)\n",
    "        return logits, new_cache\n",
    "\n",
    "    # Initialize cache\n",
    "    cache = init_cache(model, params, input_ids.shape[0], max_new_tokens)\n",
    "    print(jax.tree.map(jnp.shape, cache))\n",
    "\n",
    "    # Prefill context\n",
    "    logits, cache = forward_step(input_ids, cache)\n",
    "\n",
    "    # Generate new tokens\n",
    "    generated_ids = []\n",
    "    current_id = input_ids[:, -1:]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, cache = forward_step(current_id, cache)\n",
    "        current_id = jnp.argmax(logits[:, -1:], axis=-1)\n",
    "        generated_ids.append(current_id)\n",
    "\n",
    "    return tokenizer.decode(jnp.concatenate(generated_ids, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/flax/core/lift.py:310: RuntimeWarning: kwargs are not supported in scan, so \"cache, decode\" is(are) ignored\n",
      "  warnings.warn(msg.format(name, ', '.join(kwargs.keys())), RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tuple arity mismatch: 0 != 2; tuple: ().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m DTYPE = jnp.float32\n\u001b[32m      8\u001b[39m model = SimpleLLM(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_len=MAX_LEN, dtype=DTYPE)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m params = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m jax.tree.map(jnp.shape, params)\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mSimpleLLM.__call__\u001b[39m\u001b[34m(self, input_ids, cache, decode)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Stack blocks\u001b[39;00m\n\u001b[32m     99\u001b[39m blocks = ScannedBlock(\n\u001b[32m    100\u001b[39m     hidden_dim=\u001b[38;5;28mself\u001b[39m.hidden_dim,\n\u001b[32m    101\u001b[39m     num_heads=\u001b[38;5;28mself\u001b[39m.num_heads,\n\u001b[32m    102\u001b[39m     dtype=\u001b[38;5;28mself\u001b[39m.dtype\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m x, cache = \u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Final output logits\u001b[39;00m\n\u001b[32m    107\u001b[39m logits = nn.Dense(\u001b[38;5;28mself\u001b[39m.vocab_size)(x)\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/jax/_src/tree_util.py:360\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.tree.map`.\"\"\"\u001b[39;00m\n\u001b[32m    359\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m all_leaves = [leaves] + [\u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten_up_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treedef.unflatten(f(*xs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*all_leaves))\n",
      "\u001b[31mValueError\u001b[39m: Tuple arity mismatch: 0 != 2; tuple: ()."
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 42\n",
    "NUM_HEADS = 4\n",
    "MAX_LEN = 1024\n",
    "DTYPE = jnp.float32\n",
    "\n",
    "model = SimpleLLM(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, max_len=MAX_LEN, dtype=DTYPE)\n",
    "params = model.init(init_rng, jnp.ones((1, 32), dtype=jnp.int32))\n",
    "\n",
    "jax.tree.map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
