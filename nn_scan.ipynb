{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flax.linen.scan module can be used to reduce the whole repeated or iterative computation graph into a single interation, therefore reduces the graph memory and compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload modules so we can edit the code and have it automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import timeit\n",
    "\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic iterative module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'block_0': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'block_1': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'block_2': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}},\n",
       "  'block_3': {'Dense_0': {'bias': (1024,), 'kernel': (1024, 1024)},\n",
       "   'self_attention': {'key': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'out': {'bias': (1024,), 'kernel': (4, 256, 1024)},\n",
       "    'query': {'bias': (4, 256), 'kernel': (1024, 4, 256)},\n",
       "    'value': {'bias': (4, 256), 'kernel': (1024, 4, 256)}}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class SimpleBlock(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, attention_mask=None):\n",
    "        out = {}\n",
    "        out[\"input_hidden_state\"] = x\n",
    "        x += nn.SelfAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            dtype=self.dtype,\n",
    "            name=\"self_attention\"\n",
    "\t\t\t\t\t)(x, attention_mask)\n",
    "        out[\"+self_attention\"] = x\n",
    "        x += nn.Dense(features=self.hidden_dim)(x)\n",
    "        out[\"+dense\"] = x\n",
    "        return x, out\n",
    "\n",
    "class MyIterativeModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [\n",
    "            SimpleBlock(\n",
    "                name=f\"block_{i}\",\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                num_heads=self.num_heads,\n",
    "                dtype=self.dtype\n",
    "\t\t\t\t\t\t) \n",
    "            for i in range(self.num_layers)  # Use num_layers instead of hardcoded 3\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = {}\n",
    "        for i, block in enumerate(self.blocks):\n",
    "          x, layer_out = block(x)\n",
    "          out[f\"block_{i}\"] = layer_out\n",
    "        return x, out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for regular apply: 255.76 ms\n",
      "Time for first JIT call: 1093.07 ms\n",
      "Average time for subsequent JIT calls: 0.77 ms\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 32\n",
    "HIDDEN_DIM = 2048\n",
    "LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "DTYPE = jnp.float32\n",
    "\n",
    "model = MyIterativeModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "dummy_input = jax.random.normal(rng, (1, SEQ_LEN, HIDDEN_DIM))\n",
    "params = model.init(init_rng, dummy_input)\n",
    "\n",
    "input = jax.random.normal(rng, (BATCH_SIZE, SEQ_LEN, HIDDEN_DIM))\n",
    "# JIT compile the model's apply function for faster execution\n",
    "jit_apply = jax.jit(model.apply)\n",
    "\n",
    "def measure_time(fn, params, input, num_runs=10):\n",
    "  total_time = timeit.timeit(\n",
    "    lambda: jax.tree.map(lambda x: x.block_until_ready(), fn(params, input)),\n",
    "    number=num_runs\n",
    "  )\n",
    "  avg_time_ms = (total_time / num_runs) * 1000  # Convert to milliseconds\n",
    "  return f\"{avg_time_ms:.2f} ms\"\n",
    "\n",
    "print(f\"Average time for regular apply: {measure_time(model.apply, params, input)}\")\n",
    "print(f\"Time for first JIT call: {measure_time(jit_apply, params, input, num_runs=1)}\") \n",
    "print(f\"Average time for subsequent JIT calls: {measure_time(jit_apply, params, input, num_runs=10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks(x)\n\u001b[32m     24\u001b[39m scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m scan_params = \u001b[43mscan_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m jax.tree.map(jnp.shape, scan_params)\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mMyScanModule.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/flax/core/axes_scan.py:165\u001b[39m, in \u001b[36mscan.<locals>.scan_fn\u001b[39m\u001b[34m(broadcast_in, init, *args)\u001b[39m\n\u001b[32m    161\u001b[39m f_flat, out_tree = jax.api_util.flatten_fun_nokwargs(\n\u001b[32m    162\u001b[39m     lu.wrap_init(broadcast_body, debug_info=debug_info), in_tree\n\u001b[32m    163\u001b[39m )\n\u001b[32m    164\u001b[39m in_pvals = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(pe.PartialVal.unknown, in_avals))\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m _, out_pvals, _ = \u001b[43mpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m out_flat = []\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pv, const \u001b[38;5;129;01min\u001b[39;00m out_pvals:\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/flax/core/axes_scan.py:135\u001b[39m, in \u001b[36mscan.<locals>.scan_fn.<locals>.body_fn\u001b[39m\u001b[34m(c, xs, init_mode)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbody_fn\u001b[39m(c, xs, init_mode=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    131\u001b[39m   \u001b[38;5;66;03m# inject constants\u001b[39;00m\n\u001b[32m    132\u001b[39m   xs = jax.tree_util.tree_map(\n\u001b[32m    133\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m ax, arg, x: (arg \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m x), in_axes, args, xs\n\u001b[32m    134\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m   broadcast_out, c, ys = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcast_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m init_mode:\n\u001b[32m    138\u001b[39m     ys = jax.tree_util.tree_map(\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m ax, y: (y \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m ()), out_axes, ys\n\u001b[32m    140\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/disks/cambrianu-interleave/envs/miniconda/envs/gemma/lib/python3.11/site-packages/flax/core/lift.py:1012\u001b[39m, in \u001b[36mscan.<locals>.inner.<locals>.scanned\u001b[39m\u001b[34m(broadcast_vars, carry, scan_variable_groups, rng_groups, args)\u001b[39m\n\u001b[32m   1008\u001b[39m   variable_groups, rng_groups = data_transform(\n\u001b[32m   1009\u001b[39m       variable_groups, rng_groups\n\u001b[32m   1010\u001b[39m   )\n\u001b[32m   1011\u001b[39m scope = scope_fn(variable_groups, rng_groups)\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m c, y = fn(scope, c, *args)\n\u001b[32m   1013\u001b[39m out_vars = repack_fn(scope)\n\u001b[32m   1014\u001b[39m broadcast_vars_out = out_vars[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "class MyScanModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    dtype: jnp.dtype\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = nn.scan(\n",
    "            SimpleBlock,\n",
    "            variable_axes={},  # no scan over params\n",
    "            variable_broadcast=\"params\",  # share params\n",
    "            split_rngs={\"params\": False},  # same rng for all\n",
    "            in_axes=0,  # assume input is (layers, ...)\n",
    "            out_axes=0,\n",
    "            length=self.num_layers,\n",
    "\t\t)(\n",
    "\t\t\thidden_dim=self.hidden_dim,\n",
    "\t\t\tnum_heads=self.num_heads,\n",
    "\t\t\tdtype=self.dtype\n",
    "\t\t)\n",
    "    def __call__(self, x):\n",
    "           return self.blocks(x)\n",
    "\n",
    "scan_model = MyScanModule(hidden_dim=HIDDEN_DIM, num_layers=LAYERS, num_heads=NUM_HEADS, dtype=DTYPE)\n",
    "scan_params = scan_model.init(init_rng, dummy_input)\n",
    "\n",
    "jax.tree.map(jnp.shape, scan_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Example: Distributed LLM inference with KV cachingm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
